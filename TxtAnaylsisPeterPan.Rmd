---
title: "Text Analysis of Peter Pan"
author: "Lauren Temple"
date: "12/6/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load(gutenbergr, dplyr, stringr, tidyverse, 
               tidytext, tidyr, scales, ggplot2, wordcloud,
               tnum, magrittr)
```

#Part One
```{r}
#Peter Pan By 
#Download Peter Pan from Gutenberg Project
peter_pan <- gutenberg_download(16)
```


# Part Two

## Tidy Peter Pan

```{r}
tidy_pp <- peter_pan %>%
  mutate(
    linenumber = row_number(),
    chapter = cumsum(str_detect(text, 
                                regex("^chapter [\\divxlc]", 
                                      ignore_case = TRUE)))) %>%
  ungroup() %>%
  unnest_tokens(word, text)
# Begin from first chapter
tidy_pp <- filter(tidy_pp, chapter >=1)
```

# Sentiment Analysis with tidy data

## Bing Sentiment
```{r}
bing_negative <- get_sentiments("bing") %>% 
  filter(sentiment == "negative")
bing_positive <- get_sentiments("bing") %>%
  filter(sentiment == "positive")

negative <- tidy_pp %>%
  inner_join(bing_negative) %>%
  count(word, sort = TRUE)
positive <- tidy_pp %>%
  inner_join(bing_positive) %>%
  count(word, sort = TRUE)

pp_sent_bing <- tidy_pp %>% 
  inner_join(get_sentiments("bing")) %>% 
               count(index = linenumber %/% 100, sentiment) %>%
               pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%
               mutate(sentiment = positive - negative)
tibble(pp_sent_bing)
```

### Graph of the bing sentiment analysis

```{r}
ggplot(pp_sent_nrc, aes(index, sentiment)) +
  geom_col(show.legend = FALSE) 
```

### Comparing the three different sentiment dictionaries

```{r}
afinn <- tidy_pp %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(index = linenumber %/% 100) %>% 
  summarise(sentiment = sum(value)) %>% 
  mutate(method = "AFINN")

bing_and_nrc <- bind_rows(
  tidy_pp %>% 
    inner_join(get_sentiments("bing")) %>%
    mutate(method = "Bing et al."),
  tidy_pp %>% 
    inner_join(get_sentiments("nrc") %>% 
                 filter(sentiment %in% c("positive", 
                                         "negative"))
    ) %>%
    mutate(method = "NRC")) %>%
  count(method, index = linenumber %/% 100, sentiment) %>%
  pivot_wider(names_from = sentiment,
              values_from = n,
              values_fill = 0) %>% 
  mutate(sentiment = positive - negative)

bind_rows(afinn, 
          bing_and_nrc) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")
```

```{r}
nrc_negative <- get_sentiments("nrc") %>% 
  filter(sentiment == "negative")
nrc_positive <- get_sentiments("nrc") %>%
  filter(sentiment == "positive")

negative <- tidy_pp %>%
  inner_join(nrc_negative) %>%
  count(word, sort = TRUE)
positive <- tidy_pp %>%
  inner_join(nrc_positive) %>%
  count(word, sort = TRUE)

pp_sent_nrc <- tidy_pp %>% 
  inner_join(get_sentiments("nrc")) %>% 
               count(index = linenumber %/% 100, sentiment) %>%
               pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%
               mutate(sentiment = positive - negative)
```
### Table of all three lexicon sentiment percentages by index
```{r, include= FALSE}
tidy_nrc <- pp_sent_nrc [c(1, 12)]
tidy_nrc <- rename(tidy_nrc, 'nrc sentiment' = 'sentiment')
tidy_bing <- pp_sent_bing [c(1,4)]
tidy_bing <- rename(tidy_bing, 'bing sentiment' = 'sentiment')
tidy_afinn <- afinn [c(1,2)]
tidy_afinn <- rename(tidy_afinn, 'afinn sentiment' = 'sentiment')

table_all_sentiment <- left_join(tidy_nrc, tidy_bing, by= "index")
table_all_sentiment <- left_join(table_all_sentiment, tidy_afinn, by= "index")
```

```{r}
table_all_sentiment
```

```{r}
range(table_all_sentiment$`nrc sentiment`)
range(table_all_sentiment$`bing sentiment`)
range(table_all_sentiment$`afinn sentiment`)
```
```{r}
sd(table_all_sentiment$`nrc sentiment`)
sd(table_all_sentiment$`bing sentiment`)
sd(table_all_sentiment$`afinn sentiment`)
```

- All three lexicons show similar trends through the book
- The afinn sentiment shows the largest range of sentiment as well as the greatest standard deviation

### Ratio of negative words in each chapter

```{r}
bingnegative <- get_sentiments("bing") %>% 
  filter(sentiment == "negative")

wordcounts <- tidy_pp %>%
  group_by(chapter) %>%
  summarize(words = n())

pp_neg_tibble <- tidy_pp %>%
  semi_join(bingnegative) %>%
  group_by(chapter) %>%
  summarize(negativewords = n()) %>%
  left_join(wordcounts, by = "chapter") %>%
  mutate(ratio = negativewords/words) %>%
  ungroup()
pp_neg_tibble
```

### Ratio of positive words in each chapter

```{r}
bingpositive <- get_sentiments("bing") %>% 
  filter(sentiment == "positive")

wordcounts1 <- tidy_pp %>%
  group_by(chapter) %>%
  summarize(words = n())

pp_pos_tibble <- tidy_pp %>%
  semi_join(bingpositive) %>%
  group_by(chapter) %>%
  summarize(positivewords = n()) %>%
  left_join(wordcounts1, by = "chapter") %>%
  mutate(ratio = positivewords/words) %>%
  ungroup()
pp_pos_tibble


tidy_pp %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))
```

# Part Three

- true numbers gives you the location of each sentence
- think of true numbers like a tree diagram
- book -> chapters -> paragraphs -> sentences
- subject = book/chapter, properties= paragraph, value= sentence

## Ingest Peter Pan into TrueNumbers

```{r}
tnum.authorize("mssp1.bu.edu")
tnum.setSpace("test2")
tnum_pp <- gutenberg_download(gutenberg_id = 16)
source("Book2TN-v6A-1.R")

#injest book into true number test space 2
#tnBooksFromLines(tnum_pp, "tnum_pp/assignment4") #no chapters

#ensure that the book was injested
#tnum.getDBPathList(taxonomy = "subject", levels = 2)
```

## Begin Queries

```{r}
whole <- tnum.query("tnum_pp/assignment4/# has text", max= 2835)
whole_df <- tnum.objectsToDf(whole)
# the book repeats itself after a few chapters starting at line 564
# get to the end of the book at line 2826, for some reason we are missing a few of the last lines
whole_df <- slice(whole_df, 564:2826)
```

### Split tnum data base into chapters
- The book did not properly get ingested with chapter numbers 
- I attempted to ingest the book as a text file with <> around headings but that failed to ingest into the test space

```{r}
q111 <- tnum.query(query = "tnum_pp# has ordinal", max=500)   ## everything
df111 <- tnum.objectsToDf(q111)

## show ordered objects in document
q112 <- tnum.query("tnum_pp# has ordinal")   ## show ordered objects in document
df112 <- tnum.objectsToDf(q112)

## focus on one paragraph -- note the word count for each sentence
q3 <- tnum.query("tnum_pp/assignment4/section:0000/paragraph:0024# has count#")  # just 1 para
df3 <- tnum.objectsToDf(q3)
df3


## and now look at the text in a sentence
q1 <- tnum.query("tnum_pp/assignment4/section:0000/paragraph:0024/sentence:0003# has text")
df1 <- tnum.objectsToDf(q1)
df1

## To extract a paragraph of text
q24 <- tnum.query("tnum_pp/assignment4/section:0000/paragraph:0024/sentence# has text", max = 15)
df24 <- tnum.objectsToDf(q24)
para_text24 <- df24 %>% pull(string.value) %>% 
  str_replace_all("\"","") %>% 
  str_flatten(collapse = " ")
```

## Analysis -- sentences and paragraphs

```{r}

pq1 <- tnum.query("tnum_pp/assignment4/section:* has ordinal", max = 2835)
pqdf1 <- tnum.objectsToDf(pq1)

## Convert pqdf1 into a dataframe to reformat it for para-level analysis
## start with ordinals -- the gaps in the orginal numbering show where the headings go

bk_df <- pqdf1 %>% separate(col=subject, sep="/para", into = c("section", "para")) 

bk_df %<>% separate(col=section, sep=":", into= c("out","section"))

bk_df %<>% separate(col=para, sep="/", into=c("pars", "sent"))

bk_df %<>% separate(col=pars, sep=":", into=c("out1", "para"))

bk_df %<>% separate(col=sent, sep=":", into=c("out2", "sent"))

bk_df %<>% rename(ordinal=numeric.value)

bk_df %<>% select(section, para, sent, ordinal)

## Now the word counts

pq2 <- tnum.query("tnum_pp/assignment4/section:* has count:#", max = 2835)
pqdf2 <- tnum.objectsToDf(pq2)

bk_w_df <- pqdf2 %>% separate(col=subject, sep="e:", into=c("out", "sent1"))

bk_w_df %<>% rename(word_count = numeric.value)

bk_w_df %<>% select(sent1, word_count)

bk_df <- cbind2(bk_df, bk_w_df)

## now add the text

pq3 <- tnum.query("tnum_pp/assignment4/section:* has text", max = 2835)
pqdf3 <- tnum.objectsToDf(pq3)

bk_t_df <- pqdf3 %>% separate(col=subject, sep="e:", into=c("out", "sent1"))

bk_t_df %<>% rename(s_text = string.value)

bk_t_df %<>% select(s_text)
  
bk_df <- cbind2(bk_df, bk_t_df)


pq4 <- tnum.query("tnum_pp/assignment4/section:* has text", max = 2835)
pqdf4 <- tnum.objectsToDf(pq4)

bk_tag_df <- pqdf4 %>% select(tags)


## form paragraphs
```

## Sentimentr with  tnum

```{r}
peter_1 <- get_sentences(para_text24)

## to get sentiment scores by sentence
sentiment(peter_1)

## to get sentiment scores aggregated by paragraph
sentiment_by(peter_1)
```

```{r}
#tnum.query("tnum_pp/assignment4/section:0022/paragraph:0002/sentence:# has *")

q31 <- tnum.query("tnum_pp/assignment4/section:0000# has ordinal", max=500)
qdf31 <- tnum.objectsToDf(q31)


q30 <- tnum.query("tnum_pp/assignment4/section:0000/paragraph:0024# has * = REGEXP(\" Wendy\")")

qdf30 <- tnum.objectsToDf(q30)

tnum.tagByQuery("tnum_pp/assignment4/section:0000# has * = REGEXP(\" Wendy\")", adds=("reference:Wendy"))


# now the query for the tag gives you the same references

q31 <- tnum.query("@reference:Wendy")
qdf31 <- tnum.objectsToDf(q31)

graph4 <- tnum.makeTnumPhraseGraph(query2)

tnum.plotGraph(graph4)
```




```{r}
query3 <- tnum.query("tnum_pp/# has * = REGEXP(\"fly\")", max = 200)

tnum.tagByQuery("tnum_pp/# has * = REGEXP(\"fly\")", adds=("ref:fly"))
query4 <- tnum.query("@ref:fly")

df2 <- tnum.objectsToDf(query4)


graph2 <- tnum.makeTnumPhraseGraph(query4, "string.value")


tnum.plotGraph(graph2)
```
